# Squared ReLU (ReLU²)

Squared ReLU is an activation function defined as $f(x) = \max(0, x)^2$.

## Properties
- **Smoothness**: Unlike standard ReLU, which has a discontinuity in its derivative at 0, ReLU² is continuously differentiable.
- **Performance**: It has been empirically shown to improve convergence and performance in some large language models (e.g., PaLM, LLaMA variants).
